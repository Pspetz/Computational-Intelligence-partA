{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 04:12:24.541298: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-01 04:12:24.541323: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from sklearn import preprocessing\n",
    "from tensorflow.keras.layers import Dense,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.optimizer_v1 import SGD\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN DATA\n",
    "path = '/home/spetz/Downloads/DeliciousMIL/Data/train-data.dat'\n",
    "\n",
    "\n",
    "clean_files = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "file = open(path).readlines()\n",
    "len(file)\n",
    "\n",
    "\n",
    "clean_doc = []\n",
    "wordfreq = {}\n",
    "for doc in file:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreq.keys():\n",
    "            wordfreq[token] = 1\n",
    "        else:\n",
    "            wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "\n",
    "sentence_vectors = []\n",
    "for doc in file:\n",
    "    doc_tokens = nltk.word_tokenize(doc)\n",
    "    vec = []\n",
    "    for token in wordfreq:\n",
    "        if token in doc_tokens:\n",
    "            count = 0\n",
    "            for tok in doc_tokens:\n",
    "                if tok == token:\n",
    "                    count += 1\n",
    "            vec.append(count)\n",
    "        else:\n",
    "            vec.append(0)\n",
    "    sentence_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST-DATA\n",
    "path = '/home/spetz/Downloads/DeliciousMIL/Data/test-data.dat'\n",
    "\n",
    "clean_files = []\n",
    "df = pd.DataFrame()\n",
    "\n",
    "file = open(path).readlines()\n",
    "len(file)\n",
    "\n",
    "clean_docc = []\n",
    "wordfreqq = {}\n",
    "for doc in file:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    for token in tokens:\n",
    "        if token not in wordfreqq.keys():\n",
    "            wordfreqq[token] = 1\n",
    "        else:\n",
    "            wordfreqq[token] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist()\n",
    "sentence_vectorss = []\n",
    "for doc in file:\n",
    "    doc_tokens = nltk.word_tokenize(doc)\n",
    "    vecc = []\n",
    "    for token in wordfreqq:\n",
    "        if token in doc_tokens:\n",
    "            count = 0\n",
    "            for tok in doc_tokens:\n",
    "                if tok == token:\n",
    "                    count += 1\n",
    "            vecc.append(count)\n",
    "        else:\n",
    "            vecc.append(0)\n",
    "    sentence_vectorss.append(vecc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same length lists\n",
    "#X_train = pad_sequences(sentence_vectors , padding = 'post',maxlen=20,dtype='float32')\n",
    "#X_test = pad_sequences(sentence_vectorss , padding = 'post',maxlen=20 ,dtype='float32')\n",
    "#np\n",
    "#train_data\n",
    "X_train =np.array(sentence_vectors)\n",
    "#test data\n",
    "X_test =np.array(sentence_vectorss)\n",
    "\n",
    "#morfopoihsh \n",
    "X_train=X_train[:, :-320]\n",
    "X_test=X_test[:, :-1]\n",
    "\n",
    "#load labels\n",
    "labels_fnames = [\n",
    "            '/home/spetz/Downloads/DeliciousMIL/Data/train-label.dat',\n",
    "            '/home/spetz/Downloads/DeliciousMIL/Data/test-label.dat'\n",
    "            ]\n",
    "\n",
    "Y_train = pd.read_csv(labels_fnames[0] , delimiter = ' ', header = None)\n",
    "Y_test= pd.read_csv(labels_fnames[1], delimiter = ' ', header = None)\n",
    "\n",
    "\n",
    "#len(test_labels) 3983\n",
    "#len(train_labels) 8251\n",
    "\n",
    "\n",
    "def preprocessing(X_train,Y_train,X_test,Y_test,type=\"Normalization\"):\n",
    "\n",
    "            #NORMALIZATION#\n",
    "    if type == \"Normalization\":\n",
    "        X_train_normalized = tf.keras.utils.normalize(X_train)\n",
    "        X_test_normalized = tf.keras.utils.normalize(X_test)\n",
    "        return X_train_normalized,Y_train,X_test_normalized,Y_test\n",
    "\n",
    "            #STANDARDIZED#\n",
    "    elif type == \"Standardized\":\n",
    "        scaler = StandardScaler()\n",
    "        X_train_Standardized =scaler.fit_transform(X_train)\n",
    "        X_test_Standardized=scaler.fit_transform(X_test)\n",
    "        return X_train_Standardized,Y_train,X_test_Standardized,Y_test\n",
    "            #NORM-WITH MINMAX#\n",
    "    elif type == \"MinMax\":\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_minmax = scaler.fit_transform(X_train)\n",
    "        X_test_minmax = scaler.fit_transform(X_test)\n",
    "        return X_train_minmax , Y_train ,X_test_minmax ,Y_test\n",
    "\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D,MaxPool2D,Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.python.keras.optimizer_v1 import SGD\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(Dense(20, activation='relu', input_shape=(8251,)))\n",
    "    model.add(Dense(20, activation='sigmoid'))\n",
    "    # optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.001,momentum = 0)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model \n",
    "def create_model_mse():\n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(Dense(20, activation='relu', input_shape=(8251,)))\n",
    "    model.add(Dense(20, activation='sigmoid'))\n",
    "    # optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.001, momentum = 0)\n",
    "    model.compile(optimizer=opt,\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "def evaluate_model(X_train_normalized,Y_train,X_test_normalized,Y_test):            \n",
    "    fold_number2=0\n",
    "    fold_number = 0\n",
    "    sum_of_acc2 =0\n",
    "    sum_of_loss2 = 0\n",
    "    sum_of_acc=0\n",
    "    sum_of_loss=0\n",
    "    losses,scores,histories = list(),list(),list()\n",
    "    losses2,scores2,histories2 = list(),list(),list()\n",
    "    kfold = KFold(n_splits=5, shuffle=False, random_state=None)\n",
    "    epochs = 30\n",
    "    \n",
    "    for train_index, test_index in kfold.split(X_train_normalized,Y_train):  \n",
    "        shallow_mlp_model = create_model()\n",
    "        mse_model = create_model_mse()\n",
    "        #es=EarlyStopping(monitor='val_loss' , mode='min' , verbose=1)\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X_train_normalized[train_index,:], X_train_normalized[test_index,:]\n",
    "        y_train, y_test = Y_train.iloc[train_index],Y_train.iloc[test_index]\n",
    "    \n",
    "        #MODEL for cross-entropy\n",
    "\n",
    "        history = shallow_mlp_model.fit(X_train_normalized[train_index,:],Y_train.iloc[train_index] , epochs=epochs , validation_data=(X_test, y_test) ,verbose=1)\n",
    "        loss, val_acc = shallow_mlp_model.evaluate(X_test_normalized,Y_test,verbose=1)\n",
    "\n",
    "        #MODEL 2 for Mse\n",
    "       \n",
    "        history2 = mse_model.fit(X_train_normalized[train_index,:],Y_train.iloc[train_index] , epochs=epochs , validation_data=(X_test, y_test)  ,verbose=1)\n",
    "        loss2, val_acc2 = mse_model.evaluate(X_test_normalized,Y_test,verbose=1)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "        print(\"-\"*80)\n",
    "        ###########################\n",
    "        fold_number +=1 \n",
    "        fold_number2 +=1\n",
    "        ##########################\n",
    "        print(\" for cross entropy fold\",(fold_number),\"\\n|  loss:\" , loss, \"Accuracy:\",val_acc)\n",
    "        print(\" for Mse fold\",(fold_number2),\"\\n|  loss:\" , loss2, \"Accuracy:\",val_acc2)\n",
    "        ##########################\n",
    "        sum_of_acc += val_acc\n",
    "        sum_of_loss += loss\n",
    "        #########################\n",
    "        sum_of_loss2 +=loss2\n",
    "        sum_of_acc2 += val_acc2\n",
    "\n",
    "        scores.append(val_acc)\n",
    "        histories.append(history)\n",
    "        scores2.append(val_acc2)\n",
    "        histories2.append(history2)\n",
    "\n",
    "        print(\"-\"*80)\n",
    "        print(\"\\n Cross-Entropy:the average of the loss and acc is: \\n\",\"loss:\" , sum_of_loss/fold_number, \"\\n\" , \"Accuracy\" , sum_of_acc/fold_number,\"\\n\")\n",
    "        print(\"\\n MSE:the average of the lose and acc is: \\n\",\"loss:\" , sum_of_loss2/fold_number2, \"\\n\" , \"Accuracy\" , sum_of_acc2/fold_number2,\"\\n\")\n",
    "        \n",
    "    return history,history2\n",
    "\n",
    "\n",
    "    \n",
    "def create_model_plots(history,history2):\n",
    "        plt.figure(0)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history.history['accuracy'], label='Accuracy (train)')\n",
    "        plt.plot(history.history['val_accuracy'], label='Accuracy (test)')\n",
    "        plt.title(\"Accuracy with Cross Entropy loss\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history2.history['accuracy'], label='Accuracy (train)')\n",
    "        plt.plot(history2.history['val_accuracy'], label='Accuracy (test)')\n",
    "        plt.title(\"Accuracy with MSE loss\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # plot the cross entropy loss\n",
    "        plt.figure(1)\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(history.history['loss'], label='Cross entropy (train)')\n",
    "        plt.plot(history.history['val_loss'], label='Cross entropy (test)')\n",
    "        plt.title('Cross Entropy Evaluated')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Error value')\n",
    "        plt.legend()\n",
    "\n",
    "    # plot the mse loss\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(history2.history['loss'], label='MSE (train)')\n",
    "        plt.plot(history2.history['val_loss'], label='MSE (test)')\n",
    "\n",
    "        plt.title('MSE Evaluated')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Error value')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():\n",
    "   x_train,y_train,x_test,y_test=preprocessing(X_train,Y_train,X_test,Y_test,type=\"MinMax\")\n",
    "   h1,h2 = evaluate_model(x_train,y_train,x_test,y_test)\n",
    "   create_model_plots(h1,h2)\n",
    "run_test()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
